[
  {
    "objectID": "Speaker.html",
    "href": "Speaker.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "ğŸ”¹ ShinyConf 2025\n\nTitle: Reviewing Clinical Data Efficiently with Shiny\nğŸ“ Date: 2025-04-12\nğŸ“ Summary:\nThis presentation introduces a Shiny-based application designed to improve the efficiency of clinical data review. Traditional EDC systems often limit reviewers to viewing data one form and one patient at a time, making it difficult to cross-reference information across forms such as Adverse Events (AE), Exposure (EX), and Concomitant Medication (CM). This tool addresses that challenge by providing a user-friendly, click-driven interface that allows reviewers to select patients, filter forms and variables, and instantly visualize clinical timelines and data listings. The application maintains the original data structure, requires minimal setup by programmers, and is accessible to non-programming users. Key benefits include simultaneous multi-form data review, integrated visualizations and listings, and Excel export functionality. This tool aims to bridge communication gaps between reviewers and programmers while enhancing the speed and clarity of clinical review workflows..\nğŸ“„ Download Slides (PDF)\n\n\n\nğŸ”¹ R/Pharma 2024\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\nğŸ“ Date: 2024-10-31\nğŸ“ Summary:\nThis presentation explores how R and Shiny can enhance the review and visualization of clinical trial data. Traditional workflows often involve repeated back-and-forth verification between datasets such as SDTM, ADaM, and EDC, which is time-consuming. By leveraging Râ€™s Shiny framework, we can streamline data filtering and visualization, enabling faster and more intuitive review processes for both statisticians and medical teams. The talk highlights three key Shiny applications: tumor response visualization, patient milestone tracking, and SDTM domain review. These tools support both population-level summaries and individual-level insights. Emphasis is also placed on the importance of using CDISC-compliant data formats to standardize and simplify data handling. Finally, the integration of Shiny with Quarto is introduced as a future direction to make clinical data more accessible to non-programmers, improving data transparency and efficiency in reporting.\nğŸ“„ Download Slides (PDF)\n\n\n\nğŸ”¹ Pharmasug 2018\n\nTitle: Using Shiny to Clearly Present Clinical Results with CDISC-Compliant Dataset\nğŸ“ Date: 2018-08-31\nğŸ“ Summary:\nThis paper discusses how to enhance programming quality and efficiency in clinical trials under tight deadlines, especially within CROs. It emphasizes the importance of â€œfirst-time quality,â€ defined as the quality of deliverables before QC review. The author argues that poor initial quality leads to higher correction costs and delays, despite common practices like SOPs, validated macros, and training. A structured QC plan is essential but resource-intensive. The paper highlights factors affecting quality, including unfamiliarity with study design, insufficient task understanding, and client complexity. To address this, the author proposes a â€œFirst-Time Quality Scaleâ€ that evaluates programmers based on QC comments, client feedback, and satisfaction. High-scoring individuals can be assigned to time-sensitive projects, maximizing both quality and speed. The paper concludes that focusing on first-time quality can reduce repetitive work, save resources, and improve overall outcomes, aligning with the increasing demand for rapid and accurate clinical data processing.\nğŸ“„ Download Article (PDF)"
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å­¸ç¿’å¿ƒå¾—ç¨‹å¼è¨­è¨ˆèˆ‡-ai-å·¥å…·-learning-reflections-programming-and-ai-tools",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å­¸ç¿’å¿ƒå¾—ç¨‹å¼è¨­è¨ˆèˆ‡-ai-å·¥å…·-learning-reflections-programming-and-ai-tools",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "ğŸ§  å­¸ç¿’å¿ƒå¾—ï¼šç¨‹å¼è¨­è¨ˆèˆ‡ AI å·¥å…· | Learning Reflections: Programming and AI Tools",
    "text": "ğŸ§  å­¸ç¿’å¿ƒå¾—ï¼šç¨‹å¼è¨­è¨ˆèˆ‡ AI å·¥å…· | Learning Reflections: Programming and AI Tools\næˆªè‡³ 2025 å¹´å¹´ä¸­ï¼ŒAI å·¥å…·çš„ç™¼å±•ç›¸è¼ƒæ–¼ 2024 å¹´åˆå‘ˆç¾å‡ºå…¨æ–°çš„é¢è²Œã€‚\né€™æ¬¡æœ‰å¹¸åƒåŠ ç”±å°ç£ äº”å€å­¸é™¢ èˆ‰è¾¦çš„ AI å·¥å…·æ‡‰ç”¨åˆ†äº«æœƒï¼Œç²å¾—è¨±å¤šå¯¶è²´çš„è§€å¿µèˆ‡å¯¦æˆ°ç¶“é©—ï¼Œè®“æˆ‘åœ¨é€™å€‹ AI å¿«é€Ÿæ¼”é€²çš„éšæ®µä¸­ï¼Œå¾—ä»¥æ·±å…¥è§€å¯Ÿèˆ‡é«”é©—ä¸åŒå·¥å…·çš„æ‡‰ç”¨æ–¹å¼ã€‚\n\nAs of mid-2025, the development of AI tools has taken on a new look compared to 2024.\nI had the opportunity to attend an AI tools application sharing session organized by äº”å€å­¸é™¢ in Taiwan, and gained many valuable ideas and hands-on insights. In this rapidly evolving era of AI, this experience allowed me to better explore and try out various tool applications."
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#ç™¼å±•èˆ‡é™åˆ¶ä¸¦å­˜-coexistence-of-advancement-and-limitations",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#ç™¼å±•èˆ‡é™åˆ¶ä¸¦å­˜-coexistence-of-advancement-and-limitations",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "âš–ï¸ ç™¼å±•èˆ‡é™åˆ¶ä¸¦å­˜ | Coexistence of Advancement and Limitations",
    "text": "âš–ï¸ ç™¼å±•èˆ‡é™åˆ¶ä¸¦å­˜ | Coexistence of Advancement and Limitations\n\nç¨‹å¼é–‹ç™¼çš„æµç¨‹æ­£åœ¨è½‰è®Š: å¾éå»æ¯ä¸€è¡Œç¨‹å¼ç¢¼éƒ½éœ€ç”±é–‹ç™¼è€…è¦ªè‡ªæ’°å¯«ï¼Œåˆ°å¦‚ä»Šï¼ŒAI æˆç‚ºå¾—åŠ›çš„å”ä½œåŠ©æ‰‹ï¼Œé¡¯è‘—æå‡é–‹ç™¼æ•ˆç‡èˆ‡ç”¢é‡ã€‚å…¶ä¸­ä¸€ä½è¬›è€…åˆ†äº«ï¼Œä»–åœ¨èˆ‡ AI å”ä½œçš„æƒ…æ³ä¸‹ï¼Œä¸€å€‹æœˆå°±èƒ½ç”¢å‡ºæ•¸è¬è¡Œç¨‹å¼ç¢¼ï¼Œé€™åœ¨ä»¥å¾€å¹¾ä¹æ˜¯é›£ä»¥æƒ³åƒçš„æ•ˆç‡ã€‚\nThe software development process is evolving - from the days when every line of code was written manually, to now where AI acts as a capable assistant, greatly improving speed and productivity. One speaker shared that, with the help of AI, they were able to generate tens of thousands of lines of code within a single month - something hardly imaginable in the past.\nç„¶è€Œï¼ŒAI å·¥å…·åœ¨æ‡‰ç”¨éç¨‹ä¸­ä»å­˜åœ¨ä¸å°‘æŒ‘æˆ°ï¼Œä¾‹å¦‚ï¼š\n\nç¨‹å¼ç¢¼ç¼ºä¹ç›®æ¨™ï¼Œèƒ¡äº‚æ’°å¯«å…§å®¹\nå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ¥è­˜æœªæ›´æ–°åˆ°ç•¶ä¸‹çš„æ™‚é–“é»\nå®¹æ˜“å¿˜è¨˜ä¸Šä¸‹æ–‡å°è©±\nToken æ•¸é™åˆ¶å¸¶ä¾†ä¸Šä¸‹æ–‡ç“¶é ¸\n\n\nHowever, there are still several challenges when applying AI tools:\n\nCode may lack direction and be generated randomly\nLLMs are not trained with the most up-to-date data\nThe AI can easily forget previous conversations\nToken limits can cause loss of context"
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#è§£æ±º-ai-å·¥å…·é™åˆ¶çš„ä¸‰å€‹é—œéµåšæ³•-three-key-strategies-to-overcome-ai-limitations",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#è§£æ±º-ai-å·¥å…·é™åˆ¶çš„ä¸‰å€‹é—œéµåšæ³•-three-key-strategies-to-overcome-ai-limitations",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "ğŸ”§ è§£æ±º AI å·¥å…·é™åˆ¶çš„ä¸‰å€‹é—œéµåšæ³• | Three Key Strategies to Overcome AI Limitations",
    "text": "ğŸ”§ è§£æ±º AI å·¥å…·é™åˆ¶çš„ä¸‰å€‹é—œéµåšæ³• | Three Key Strategies to Overcome AI Limitations\né€™å ´åˆ†äº«æœƒæå‡ºäº†è¨±å¤šå¯¦ç”¨ç­–ç•¥ï¼Œæˆ‘å°‡å…¶æ­¸ç´ç‚ºä¸‰å€‹æ ¸å¿ƒé‡é»ï¼š\nThe session provided several practical strategies. I summarize them into three key actions:\n\n1ï¸âƒ£ å…ˆç†è§£ä»»å‹™ï¼Œå†æ‹†è§£è¡Œå‹• | Understand the task, then break it down\nä½¿ç”¨ AI æ’°å¯«ç¨‹å¼å‰ï¼Œæ‡‰å…ˆé‡æ¸…æ•´é«”ä»»å‹™å…§å®¹ï¼Œæ’°å¯« PRDï¼ˆç”¢å“éœ€æ±‚æ–‡ä»¶ï¼‰ï¼Œå†å°‡ä»»å‹™åˆ‡åˆ†æˆå¤šå€‹å°ç›®æ¨™ï¼Œé€æ­¥äº¤ç”± AI åŸ·è¡Œã€‚é€™ä¸åƒ…èƒ½æå‡æˆåŠŸç‡ï¼Œä¹Ÿèƒ½æ›´æœ‰ç³»çµ±åœ°æ§ç®¡é–‹ç™¼é€²åº¦èˆ‡å“è³ªã€‚\nBefore asking AI to write code, clarify the overall goal and prepare a PRD (Product Requirements Document). Then break down the task into smaller sub-tasks, and let AI handle them step by step. This ensures both higher success rates and more manageable project progress.\n\n\n2ï¸âƒ£ æ˜ç¢ºå®šç¾© Roleã€Rules èˆ‡ Prompt | Clearly define Role, Rules, and Promp\né€™éƒ¨åˆ†æ˜¯æˆ‘å€‹äººæ”¶ç©«æœ€å¤§çš„å€å¡Šï¼Œå»ºè­°é–‹ç™¼è€…å¯å¾ä»¥ä¸‹ä¸‰é»è‘—æ‰‹ï¼š\nThis was the most valuable part for me. Developers can start by defining the following:\n\nè§’è‰²ï¼š\nå®šç¾© AI æ¨¡å‹åœ¨å°è©±ä¸­çš„å°ˆæ¥­çŸ¥è­˜èˆ‡èªè¨€ç¿’æ…£ã€‚ä¾‹å¦‚ï¼šé™å®šä½¿ç”¨ JavaScript æ’°å¯«ï¼Œä¸”å°ˆæ³¨æ–¼å‰ç«¯é–‹ç™¼çš„ç›¸é—œé‚è¼¯ï¼Œå¹«åŠ© AI æ›´èšç„¦ç”¢å‡ºæ–¹å‘ã€‚\nRole:\nDefine the modelâ€™s expertise and response tone. For example, restrict output to JavaScript, focusing on frontend logic, so the AI stays on track.\nè¦å‰‡ï¼š\né™¤äº†è¦æ˜ç¢ºå®šç¾©å°ˆæ¡ˆçš„ç¨‹å¼é¢¨æ ¼èˆ‡æ¶æ§‹ï¼Œæ›´è¦è¨­å®šã€Œä¸å…è¨±åŸ·è¡Œçš„è¡Œç‚ºã€ï¼Œä¾‹å¦‚ï¼šå˜—è©¦å…©æ¬¡å¤±æ•—å¾Œæ‡‰ä¸­æ­¢ä¸¦å›å ±äººå“¡; åš´ç¦åŸ·è¡Œ rm -rf ç­‰å±éšªæŒ‡ä»¤.\né€™äº›è¦å‰‡å¯æ ¹æ“šå…¬å¸æ¨™æº–ã€åœ˜éšŠé¢¨æ ¼æˆ–å€‹äººé–‹ç™¼ç¿’æ…£è‡ªè¡Œåˆ¶å®šï¼Œä¹Ÿèƒ½åƒè€ƒä»¥ä¸‹è³‡æºèª¿æ•´ï¼š\n\nCursor Rules Directory\nGitHub Copilot Best Practices\n\nRules:\nBesides coding style and project structure, define things AI should NOT do, such as: Stop and report after two failed attempts, Never run dangerous commands like rm -rf\nThese rules can be based on company standards, team preferences, or personal habits. We can also refer to these resources for inspiration:\n\nCursor Rules Directory\nGitHub Copilot Best Practices\n\næç¤ºèªï¼š\né‡é»ä¸åœ¨ã€Œå¥—ç”¨ç¯„æœ¬ã€ï¼Œè€Œæ˜¯æ¸…æ¥šè¡¨é”éœ€æ±‚ã€‚åªè¦æ˜ç¢ºèªªå‡ºä½ è¦ä»€éº¼ã€ä½¿ç”¨å“ªç¨®èªè¨€ã€é æœŸçš„è¼¸å‡ºå½¢å¼ï¼ŒAI å°±èƒ½æœ‰æ•ˆç†è§£èˆ‡åŸ·è¡Œã€‚\nPrompt:\nThe key is to clearly state your needs. No need to memorize templates and just explain what you want, which language to use, and what output you expect.\n\n\n\n3ï¸âƒ£ å°è©±å…§å®¹éé•·æ™‚ï¼Œå–„ç”¨æ‘˜è¦èˆ‡ä¸Šä¸‹æ–‡å¼•ç”¨ | Manage long conversations with summaries and context reference\nç•¶èˆ‡ AI å°è©±æ™‚é–“æ‹‰é•·ï¼Œå®¹æ˜“è¶…å‡º token é™åˆ¶ã€‚\nå»ºè­°å°‡å°è©±æ‘˜è¦ç´€éŒ„ç‚º .md æª”æ¡ˆï¼Œæˆ–ä½¿ç”¨ AI å·¥å…·çš„è¨˜æ†¶åŠŸèƒ½ï¼ˆå¦‚ Cursor çš„ @file æ¨™è¨˜ï¼‰ï¼Œåœ¨æ–°å°è©±ä¸­å¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œè®“ AI å»¶çºŒæ€è·¯ï¼Œæå‡æ•´é«”å”ä½œå“è³ªã€‚\nWhen conversations get too long, you may hit token limits.\nTo manage this, summarize the key points into .md files or use memory features (like Cursorâ€™s @file tag) to carry context across sessions.\nThis helps maintain continuity and improves AI collaboration quality."
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å·¥å…·é¸æ“‡çš„å¤šå…ƒèˆ‡ç­–ç•¥-choosing-the-right-tools",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å·¥å…·é¸æ“‡çš„å¤šå…ƒèˆ‡ç­–ç•¥-choosing-the-right-tools",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "ğŸ§° å·¥å…·é¸æ“‡çš„å¤šå…ƒèˆ‡ç­–ç•¥ | Choosing the Right Tools",
    "text": "ğŸ§° å·¥å…·é¸æ“‡çš„å¤šå…ƒèˆ‡ç­–ç•¥ | Choosing the Right Tools\né€™æ¬¡åˆ†äº«æœƒä»‹ç´¹äº†è¨±å¤šå¸¸è¦‹çš„ AI coding å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š\n\nChatGPT\nGitHub Copilot\nCursor\nClaude Code\nCline / RooCode\nGemini CLI\n\næ¯å€‹å·¥å…·éƒ½æœ‰ä¸åŒç‰¹è‰²ï¼Œé¸æ“‡ä¸Šæ‡‰è¦–å€‹äººå·¥ä½œæµç¨‹èˆ‡å°ˆæ¡ˆéœ€æ±‚è€Œå®šã€‚\nAI å·¥å…·ç™¼å±•è®ŠåŒ–å¿«é€Ÿï¼Œä¸éœ€è¦è¿½æ±‚ã€Œæœ€å¥½ã€ï¼Œè€Œæ˜¯æ‰¾åˆ°æœ€é©åˆä½ ä½¿ç”¨ç¿’æ…£çš„é‚£ä¸€å€‹ã€‚ç•¶ç„¶ï¼Œæ˜¯å¦æ”¶è²»ä¹Ÿæ˜¯éœ€è¦ç´å…¥è€ƒé‡çš„å› ç´ ã€‚\nThe session introduced many popular AI coding tools, including:\n\nChatGPT\nGitHub Copilot\nCursor\nClaude Code\nCline / RooCode\nGemini CLI\n\nEach tool has its strengths. Choose based on your personal workflow and project needs.\nGiven how fast AI tools evolve, thereâ€™s no need to chase â€œthe bestâ€â€”just pick the one that fits you best. Also, remember to factor in cost differences."
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å¯¦ä¾‹å•Ÿç™¼å®šç¾©è®“-ai-ç™¼æ®å¾—æ›´å¥½-practical-insight-help-ai-help-you",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#å¯¦ä¾‹å•Ÿç™¼å®šç¾©è®“-ai-ç™¼æ®å¾—æ›´å¥½-practical-insight-help-ai-help-you",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "ğŸ§ª å¯¦ä¾‹å•Ÿç™¼ï¼šå®šç¾©è®“ AI ç™¼æ®å¾—æ›´å¥½ | Practical Insight: Help AI Help You",
    "text": "ğŸ§ª å¯¦ä¾‹å•Ÿç™¼ï¼šå®šç¾©è®“ AI ç™¼æ®å¾—æ›´å¥½ | Practical Insight: Help AI Help You\né€é Cursor é¦–å¸­å·¥ç¨‹å¸« Ryo Lu çš„ç¶“é©—åˆ†äº« (https://x.com/ryolu_/status/1914384195138511142)ï¼Œæ›´åŠ æ·±æˆ‘å° Role èˆ‡ Rules è¨­å®šçš„é‡è¦æ€§ç†è§£ã€‚ä»–å¼·èª¿ï¼šã€ŒAI æ˜¯æ‰‹ï¼Œä¸æ˜¯è…¦ã€‚ä½ è¦æ•™æœƒå®ƒï¼Œæ‰æœƒæˆç‚ºç¥éšŠå‹ã€‚ã€\nThrough the experience shared by Cursorâ€™s lead engineer Ryo Lu (https://x.com/ryolu_/status/1914384195138511142), I gained a deeper understanding of how defining Role and Rules can significantly improve AI output.\nåŒæ™‚ï¼Œåè¦†æ¸¬è©¦ã€å„ªåŒ– promptã€æå‡å°ç¨‹å¼çš„ç†è§£ï¼Œä»æ˜¯æ¯ä½é–‹ç™¼è€…çš„åŸºæœ¬åŠŸã€‚ç•¶ AI ç„¡æ³•çµ¦å‡ºæ­£ç¢ºç­”æ¡ˆæ™‚ï¼Œä»éœ€æœ‰èƒ½åŠ›è¦ªè‡ªä¸‹å ´æ’°å¯«ï¼Œç”šè‡³åéä¾†è®“ AI å‘ä½ å­¸ç¿’ã€‚\nRepeated testing, prompt refinement, and improving your own coding understanding remain essential. When AI fails, developers still need to write code themselvesâ€”and even help AI learn from you."
  },
  {
    "objectID": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#æ”¶ç©«-takeaways",
    "href": "blog/01-AUG-2025-Takeaways from Learning Programming and AI Tools.html#æ”¶ç©«-takeaways",
    "title": "Takeaways from Learning Programming and AI Tools - Part 1",
    "section": "ğŸ“Œ æ”¶ç©« |Takeaways",
    "text": "ğŸ“Œ æ”¶ç©« |Takeaways\n\n1ï¸âƒ£ è¬›è€…åˆ†äº«ã€ŒAI æ˜¯ Copilotï¼Œæˆ‘å€‘æ˜¯ Pilotã€ï¼Œæˆ‘èªç‚ºæˆ‘å€‘åŸ·è¡Œä¸€é …å°ˆæ¡ˆæ™‚ï¼Œå¿…é ˆèªçŸ¥åˆ° AI å§‹çµ‚åƒ…æ˜¯å·¥å…·ï¼Œæˆ‘å€‘å¿…é ˆæ¸…æ¥šæŒæ¡å°ˆæ¡ˆçš„ä¸»è»¸ï¼Œä¸¦é©æ™‚çµ¦äºˆèª¿æ•´èˆ‡åè¦†ç¢ºèªæ­£ç¢ºæ€§ã€‚\nThe speaker mentioned, â€œAI is the copilot, and we are the pilot.â€\nI believe that when working on a project, we must always remember that AI is just a tool. We, as humans, should stay in control - clearly understanding the goals, making adjustments as needed, and constantly verifying the correctness of outcomes.\n\n\n2ï¸âƒ£ ã€ŒAI ä½¿æˆ‘å€‘å¯«ç¨‹å¼çš„è‚Œè‚‰è¨˜æ†¶æ¶ˆå¤±ã€ã€ã€ŒAI è®“äººå–ªå¤±æ€è€ƒèƒ½åŠ›ã€ç­‰è² é¢ç ”ç©¶çµæœçš„å‡ºç¾ï¼Œæé†’æˆ‘å€‘å¿…é ˆä¸æ–·æ€è€ƒå¦‚ä½•æŒçºŒåœ¨è‡ªå·±çš„å°ˆæ¥­é ˜åŸŸæˆé•·ï¼Œä»¥åŠå¦‚ä½•åœ¨å¿…è¦æ™‚ä¹Ÿæœ‰èƒ½åŠ›ç¹¼çºŒç·¨è¼¯ç¨‹å¼ã€‚\nResearch has raised concerns that AI may cause us to lose our â€œmuscle memoryâ€ for coding and weaken our problem-solving abilities. This reminds us to keep improving in our fields, and ensure we can still code when needed.\næˆ‘èªç‚ºï¼Œå¶çˆ¾éœ€è¦åœä¸‹æ“ä½œå”ä½œå·¥å…·ï¼Œçœ‹çœ‹é€™äº› AI å·¥å…·ç”¢å‡ºçš„æµç¨‹èˆ‡ç¨‹å¼å…§å®¹ï¼Œè‡ªå·±æ˜¯å¦èƒ½å¤§éƒ¨åˆ†æŒæ¡ï¼›ç”šè‡³è‡ªå·±æ˜¯å¦å¯ä»¥åœ¨ä¸ä¾è³´å·¥å…·çš„ç‹€æ³ä¸‹ï¼Œç”¢å‡ºä¸€æ¨£çš„çµæœã€‚æ›å¥è©±èªªï¼Œè®“è‡ªå·±ç¶­æŒä¸€å®šçš„æŒæ¡åº¦æ˜¯å¾ˆé‡è¦çš„ã€‚\nSometimes, itâ€™s good to pause and review the AIâ€™s output:\nâ€œDo I understand the logic? Could I write this myself without the tool?â€\nMaintaining a certain level of independence is essential.\n\n\n3ï¸âƒ£ AI æ™‚ä»£ä¸‹ï¼ŒæŠ€è¡“çš„æ›´æ–°ååˆ†å¿«é€Ÿï¼Œè‡ªå·±èˆ‡é€™äº›å·¥å…·ä¹‹é–“å„æ‰®æ¼”çš„è§’è‰²ä¹Ÿä¸æœƒä¸€æˆä¸è®Šã€‚æŒçºŒäº†è§£æ½®æµçš„è®ŠåŒ–ä»¥åŠé€éé‚è¼¯æ€è€ƒè®“è‡ªå·±å·¥ä½œæ›´æœ‰æ•ˆç‡ï¼Œæ˜¯ç¶­æŒç«¶çˆ­åŠ›çš„é‡é»ä¹‹ä¸€ã€‚\nIn the age of AI, technologies are evolving rapidly, and our roles in relation to these tools are also shifting.ã€€Staying up-to-date with trends and using logical thinking to work more efficiently is one of the keys to staying competitive.\n\n\n4ï¸âƒ£ ç¹¼çºŒæ¢ç´¢æœ‰åƒ¹å€¼çš„é ˜åŸŸçŸ¥è­˜ã€‚AI å¹«æˆ‘å€‘çœå»è¨±å¤šæ’°å¯«ç¨‹å¼çš„æ™‚é–“ï¼Œæˆ‘å€‘å¯ä»¥åˆ©ç”¨é€™äº›æ™‚é–“å»æ¢ç´¢å°ˆæ¥­é ˜åŸŸçš„ç´°ç¯€ã€‚é€™å°‡è®“æˆ‘å€‘æå‡ºæ›´æ˜ç¢ºä¸”æ›´å‰µæ–°çš„æƒ³æ³•ï¼Œé€²è€Œè®“ AI å·¥å…·å¹«åŠ©æˆ‘å€‘å‰µé€ å‡ºæ›´æœ‰åƒ¹å€¼çš„æˆæœã€‚\nWe should use the time saved by AI coding to explore deeper domain knowledge. With better understanding, we can ask smarter questions and give clearer instructions - enabling AI to help us create results that are more valuable and more innovative."
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "href": "blog/23-JUN-2024-lapply_gsub.html#introduction",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ” Introduction",
    "text": "ğŸ” Introduction\nIn data analysis, we often encounter situations where we need to perform the same transformation or calculation on multiple datasets â€” such as datasets grouped by age, treatment, or study phase.\nRather than repeating the same code block over and over, R provides a more efficient, less error-prone solution: the lapply() function."
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "href": "blog/23-JUN-2024-lapply_gsub.html#why-use-lapply",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "âœ¨ Why Use lapply()?",
    "text": "âœ¨ Why Use lapply()?\nWhen you have more than 5 or even 10 datasets to process, using a loop or manually running the same function can quickly become tedious and risky. By placing all target datasets in a list and applying a custom function via lapply(), you can:\n\nMinimize code repetition\n\nReduce the chance of mistakes\n\nImprove code scalability and clarity"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "href": "blog/23-JUN-2024-lapply_gsub.html#example-adjust-units-and-calculate-bmi",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ”§ Example: Adjust Units and Calculate BMI",
    "text": "ğŸ”§ Example: Adjust Units and Calculate BMI\nLetâ€™s say you have datasets containing weight (in å…¬æ–¤) and height (in å…¬å°º) for different age groups. The goal is to:\n\nConvert the units from Chinese characters to standard abbreviations (kg, m)\n\nCalculate the Body Mass Index (BMI) using the formula:\n\n\\[\n\\text{BMI} = \\frac{\\text{weight (kg)}}{\\text{height (m)}^2}\n\\]\n\nğŸ“¦ Step 1: Create Sample Datasets\n# Sample data for three age groups\ngroup1 &lt;- data.frame(Height = c(\"1.65å…¬å°º\", \"1.70å…¬å°º\"), Weight = c(\"60å…¬æ–¤\", \"65å…¬æ–¤\"))\n\ngroup2 &lt;- data.frame(Height = c(\"1.60å…¬å°º\", \"1.75å…¬å°º\"), Weight = c(\"55å…¬æ–¤\", \"70å…¬æ–¤\"))\n\ngroup3 &lt;- data.frame(Height = c(\"1.72å…¬å°º\", \"1.80å…¬å°º\"), Weight = c(\"68å…¬æ–¤\", \"75å…¬æ–¤\"))\n# Combine them into a list\ngroups &lt;- list(group1, group2, group3)\nğŸ”§ Step 2: Define a Custom Function\n# Function to convert units and calculate BMI\nreplace_function &lt;- function(df) {\n  df$Height &lt;- as.numeric(gsub(\"å…¬å°º\", \"\", df$Height))  # Remove 'å…¬å°º' and convert to numeric\n  df$Weight &lt;- as.numeric(gsub(\"å…¬æ–¤\", \"\", df$Weight))  # Remove 'å…¬æ–¤' and convert to numeric\n  df$BMI &lt;- round(df$Weight / (df$Height^2), 1)         # Calculate BMI\n  return(df)\n}\nğŸš€ Step 3: Apply the Function Using lapply()\n# Apply the same function to all groups\nresults &lt;- lapply(groups, replace_function)\n\n# Preview the result for group1\nresults[[1]]\nâœ… Output (example)\n  Height Weight  BMI\n1   1.65     60  22.0\n2   1.70     65  22.5"
  },
  {
    "objectID": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "href": "blog/23-JUN-2024-lapply_gsub.html#final-thoughts",
    "title": "Efficiently Apply the Same Function to Multiple Datasets in R",
    "section": "ğŸ’¡ Final Thoughts",
    "text": "ğŸ’¡ Final Thoughts\nThe combination of lapply() and gsub() demonstrates a powerful pattern in R: clean, consistent, and reproducible operations across datasets.\nWhether youâ€™re dealing with different demographic groups or multiple study arms, putting your datasets in a list and defining a reusable function can save time and prevent mistakes â€” especially in clinical trial data preparation or large-scale reporting tasks.\nHappy coding!"
  },
  {
    "objectID": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#introduction",
    "href": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#introduction",
    "title": "Two Simple Ways to Fill Dummy Data into the Right Rows",
    "section": "Introduction",
    "text": "Introduction\nWhen preparing statistical summaries, we often need to create dummy data to maintain consistent structures.For example, ensuring all combinations of categorical variables appear in a summary table. This post introduces two simple approaches to fill dummy rows into your dataset using R and the tidyverse."
  },
  {
    "objectID": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#example-dataset",
    "href": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#example-dataset",
    "title": "Two Simple Ways to Fill Dummy Data into the Right Rows",
    "section": "Example Dataset",
    "text": "Example Dataset\nLetâ€™s start by creating a sample dataset:\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.2.0     âœ” readr     2.2.0\nâœ” forcats   1.0.1     âœ” stringr   1.6.0\nâœ” ggplot2   4.0.2     âœ” tibble    3.3.0\nâœ” lubridate 1.9.5     âœ” tidyr     1.3.2\nâœ” purrr     1.2.0     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(2025)\n\ntemp1 &lt;- tibble(\n  treatment = sample(c(\"g1\", \"g2\"), size = 10, replace = TRUE),\n  sub       = sample(c(\"x1\", \"x2\", \"x3\"), size = 10, replace = TRUE),\n  id        = 1:10\n)\n\ntemp1\n\n# A tibble: 10 Ã— 3\n   treatment sub      id\n   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n 1 g1        x1        1\n 2 g2        x3        2\n 3 g2        x3        3\n 4 g2        x3        4\n 5 g1        x1        5\n 6 g1        x3        6\n 7 g2        x3        7\n 8 g1        x3        8\n 9 g2        x3        9\n10 g1        x2       10\n\n\nThis dataset contains three variables:\n\ntreatment with values â€œg1â€ and â€œg2â€,\nsub with values â€œx1â€, â€œx2â€, and â€œx3â€,\nid as a sequential index.\n\nğŸ‘‰ Now imagine we need to count records by treatment and sub, but we also want to include a new category â€” y1 â€” under sub, even if it doesnâ€™t appear in the data. How can we achieve that?"
  },
  {
    "objectID": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#method-1-using-crossing",
    "href": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#method-1-using-crossing",
    "title": "Two Simple Ways to Fill Dummy Data into the Right Rows",
    "section": "Method 1 â€” Using crossing()",
    "text": "Method 1 â€” Using crossing()\nThis approach manually builds a dummy dataset and merges it with the existing summary.\nSteps:\n1ï¸âƒ£ Obtain unique treatment values.\n2ï¸âƒ£ Create all desired combinations using crossing().\n3ï¸âƒ£ Combine the dummy and original counts.\n\n# Count the original combinations\n\ntemp2 &lt;- temp1 %&gt;%\ncount(treatment, sub, name = \"count\")\n\n# Extract unique treatments\n\ntemp2_nodup &lt;- temp2 %&gt;%\n  arrange(treatment, sub, count) %&gt;%\n  group_by(treatment) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  select(treatment) %&gt;%\n  ungroup()\n\n\n# Create dummy data with all sub categories, including y1\n\ndummy &lt;- crossing(\n  temp2_nodup,\n  sub = c(\"x1\", \"x2\", \"x3\", \"y1\"),\n  count = 0\n)\n\n# Combine and keep the latest record per combination\n\ntemp3 &lt;- temp2 %&gt;%\n  rbind(dummy) %&gt;%\n  arrange(treatment, sub, count) %&gt;%\n  group_by(treatment, sub) %&gt;%\n  mutate(id1 = row_number(), id2 = n()) %&gt;%\n  filter(row_number() == n()) %&gt;%\n  select(-matches(\"^id\")) %&gt;%\n  ungroup()\n\ntemp3\n\n# A tibble: 8 Ã— 3\n  treatment sub   count\n  &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n1 g1        x1        2\n2 g1        x2        1\n3 g1        x3        2\n4 g1        y1        0\n5 g2        x1        0\n6 g2        x2        0\n7 g2        x3        5\n8 g2        y1        0"
  },
  {
    "objectID": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#method-2-using-complete",
    "href": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#method-2-using-complete",
    "title": "Two Simple Ways to Fill Dummy Data into the Right Rows",
    "section": "Method 2 â€” Using complete()",
    "text": "Method 2 â€” Using complete()\nA cleaner and more concise way uses tidyr::complete(), which automatically fills missing combinations.\nSteps:\n1ï¸âƒ£ Count existing records by group.\n2ï¸âƒ£ Use complete() to add the missing combinations.\n\n\nway2 &lt;- temp1 %&gt;%\n  count(treatment, sub, name = \"count\") %&gt;%\n  complete(\n    treatment,\n    sub = c(\"x1\", \"x2\", \"x3\", \"y1\"),\n    fill = list(count = 0)\n    )\n\nway2 \n\n# A tibble: 8 Ã— 3\n  treatment sub   count\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n1 g1        x1        2\n2 g1        x2        1\n3 g1        x3        2\n4 g1        y1        0\n5 g2        x1        0\n6 g2        x2        0\n7 g2        x3        5\n8 g2        y1        0"
  },
  {
    "objectID": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#conclusion",
    "href": "blog/12-Nov-2025-Two Simple Ways to Fill Dummy Data into the Right Rows.html#conclusion",
    "title": "Two Simple Ways to Fill Dummy Data into the Right Rows",
    "section": "ğŸ¨Conclusion",
    "text": "ğŸ¨Conclusion\nThis is a common requirement in data processing â€” ensuring all expected categories appear in your summaries or reports. Both crossing() and complete() from the tidyverse provide efficient ways to generate dummy data and maintain structural integrity.\n\nUse crossing() when you need full control over combinations and want to manually build the structure.\nUse complete() for a more concise, declarative approach that integrates naturally into tidy pipelines.\n\nIn short, crossing() gives you full control to build combinations manually, while complete() offers a cleaner, automatic way to fill missing categories in your data."
  },
  {
    "objectID": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html",
    "href": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html",
    "title": "A Journey into Data Visualization: From ggplot2 Techniques to Visual Design",
    "section": "",
    "text": "ğŸ¯ ggplot2 & Data Visualization Design\nBased on â€œData Visualization in R Using ggplot2 & Friendsâ€ by Statistics Globe / Joachim Schork and â€œFundamentals of Data Visualizationâ€ by Claus O. Wilke,\nI embarked on a 30-day data visualization challenge â€” a journey that gradually shifted my focus from technical plotting to visual thinking.\nThis challenge was part of the 2025 iThome Ironman Competition in Taiwan,\nwhere participants commit to publishing one IT-related article every day for 30 consecutive days.\nYou can view my full series here: iThome Ironman Challenge â€“ 30 Days of ggplot2.\nMy process followed a simple rhythm:\nThis rhythm turned out to be one of the most effective ways to internalize new skills and transform passive knowledge into creative output. Every day, I learned something new about how data could be shaped, expressed, and communicated visually."
  },
  {
    "objectID": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#the-grammar-of-graphics-mindset",
    "href": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#the-grammar-of-graphics-mindset",
    "title": "A Journey into Data Visualization: From ggplot2 Techniques to Visual Design",
    "section": "ğŸ§© The Grammar of Graphics Mindset",
    "text": "ğŸ§© The Grammar of Graphics Mindset\nAt its core, ggplot2 embodies the Grammar of Graphics â€” a philosophy of building visualizations through syntax, layer by layer.\nIt reminds me of SAS PROC SGPLOT and TEMPLATE:\nboth rely on layered composition, where each layer is independent yet harmonized.\nConnecting data to visuals is not merely about stacking code â€”\nitâ€™s about understanding how data structure meets visual semantics.\nThrough exploring the philosophy of aes() and the logic behind each geom_xx() layer,\nI came to realize that data visualization is not just a skill â€” itâ€™s a way of thinking.\nThroughout the series, I used real-world open datasets â€” from Taiwanâ€™s energy, postal, and demographic data, to built-in R datasets â€” making each example grounded, contextual, and relatable."
  },
  {
    "objectID": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#extensions-and-ggplot2-4.0.0-highlights",
    "href": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#extensions-and-ggplot2-4.0.0-highlights",
    "title": "A Journey into Data Visualization: From ggplot2 Techniques to Visual Design",
    "section": "ğŸŒˆ Extensions and ggplot2 4.0.0 Highlights",
    "text": "ğŸŒˆ Extensions and ggplot2 4.0.0 Highlights\nBeyond the classic chart types (scatter, density, boxplot, heatmap, line chart, etc.),\nI explored several extensions that greatly expand ggplot2â€™s expressive power:\n\nggrepel â€” for non-overlapping labels\n\nggpointdensity â€” for high-density scatter plots\n\nggstatsplot â€” for integrated statistical testing\n\npatchwork â€” for elegant multi-plot layouts\n\nOne major highlight during the challenge was the release of ggplot2 4.0.0,\nwhich marked a significant evolution in the packageâ€™s design and structure.\nHere are some key updates from the official announcement:\n1ï¸âƒ£ Transition from S3 to S7 object system â€” offering stricter type validation and more flexibility for developers.\n2ï¸âƒ£ Theme system overhaul â€” introducing ink/paper/accent logic and new helpers like theme_sub_axis().\n3ï¸âƒ£ Integration of palettes and scales into themes â€” ensuring consistent and harmonious visual design across plots.\nThese updates reflect how ggplot2 continues to balance stability and innovation, remaining one of the most elegant tools in the R ecosystem."
  },
  {
    "objectID": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#balancing-color-and-design",
    "href": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#balancing-color-and-design",
    "title": "A Journey into Data Visualization: From ggplot2 Techniques to Visual Design",
    "section": "ğŸ¨ Balancing Color and Design",
    "text": "ğŸ¨ Balancing Color and Design\nAs Cara Thompson mentioned at Shiny in Production 2023,\nthe goal of visualization isnâ€™t simply to make it pretty, but to make it instantly understandable.\nColor choices, typography, and text hierarchy all serve a purpose â€”\nto guide the readerâ€™s eye, emphasize the story, and support comprehension.\n\nClarity is not a constraint â€” itâ€™s an act of design.\n\nThroughout this challenge, I found myself constantly adjusting palettes and contrasts,\nbalancing scientific precision with aesthetic intuition."
  },
  {
    "objectID": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#personal-reflection",
    "href": "blog/15-OCT-2025-A Journey into Data Visualization - From ggplot2 Techniques to Visual Design.html#personal-reflection",
    "title": "A Journey into Data Visualization: From ggplot2 Techniques to Visual Design",
    "section": "âœ¨ Personal Reflection",
    "text": "âœ¨ Personal Reflection\nThis 30-day journey reminded me that data visualization lives at the intersection of science and art.\nAccuracy ensures integrity, but aesthetics guide attention and memory.\nA great visualization doesnâ€™t just display information â€”\nit tells a story, connects people, and makes insights memorable.\nFinally, I want to express my gratitude to the R community, the iThome Ironman platform,\nand all mentors who make learning, sharing, and creating so much more accessible and inspiring.\nTheir openness is what makes this journey truly rewarding.\n\n\n_â€œData visualization is not the end of analysis â€” itâ€™s the beginning of understanding.â€"
  },
  {
    "objectID": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html",
    "href": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html",
    "title": "Colors Matter: Exploring cols4all for Better Data Visualization",
    "section": "",
    "text": "ğŸ‘‰ Question for you: What is your go-to tool for choosing color palettes in your visualizations?"
  },
  {
    "objectID": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#why-colors-matter",
    "href": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#why-colors-matter",
    "title": "Colors Matter: Exploring cols4all for Better Data Visualization",
    "section": "Why Colors Matter",
    "text": "Why Colors Matter\nIn data visualization, colors are more than decoration. They guide attention, improve readability, and even determine whether insights are understood correctly.\nThink about it: if the difference between red and green is too subtle, someone with color vision deficiency may not see the contrast at all. In that case, the message of your chart could be completely lost.\nThis is why choosing colors carefully is essentialâ€”not just for aesthetics, but also for accessibility."
  },
  {
    "objectID": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#discovering-cols4all",
    "href": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#discovering-cols4all",
    "title": "Colors Matter: Exploring cols4all for Better Data Visualization",
    "section": "Discovering cols4all",
    "text": "Discovering cols4all\nRecently, I explored the R package cols4all. It provides a rich collection of palettes and a powerful GUI for selecting the right ones.\nWith c4a_gui(), you can:\n\nFilter palettes by type: categorical, sequential, diverging, cyclic, bivariate.\n\nAdjust the number of colors depending on your needs.\n\nTest accessibility for color vision deficiencies.\n\nCompare palettes with built-in scores for contrast and colorblind-friendliness.\n\nFor anyone who needs to balance beauty with clarity, this feels like a game changer."
  },
  {
    "objectID": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#easy-integration-with-ggplot2",
    "href": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#easy-integration-with-ggplot2",
    "title": "Colors Matter: Exploring cols4all for Better Data Visualization",
    "section": "Easy Integration with ggplot2",
    "text": "Easy Integration with ggplot2\nUsing these palettes with ggplot2 is straightforward. For example:\nscale_fill_manual(values = c4a(\"brewer.set3\"))\nYou can also pick specific colors from a palette and assign them to categories, giving you both consistency and flexibility.\nPractical Value In one project, I had to display data from multiple clinical trial groups. A simple red/blue/green scheme quickly became confusing once the number of groups exceeded five.\nWith cols4all, I could identify a colorblind-friendly palette with clear contrast in just a few clicks. That meant doctors, statisticians, and non-technical stakeholders could all interpret the chart without difficulty.\nThe result? Charts that were not only professional-looking, but also universally readable.\nTakeaway What I love about cols4all:\nâœ”ï¸ It goes beyond making charts prettyâ€”it makes them understandable for everyone.\nâœ”ï¸ It saves time by avoiding endless trial-and-error in color selection.\nâœ”ï¸ It combines aesthetics with accessibility, increasing the impact of your visualizations."
  },
  {
    "objectID": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#back-to-you-what-do-you-usually-rely-on-when-choosing-colors-do-you-stick-with-rcolorbrewer-pick-manually-or-have-you-tried-cols4all",
    "href": "blog/13-Sep-2025-Exploring cols4all for Better Data Visualization.html#back-to-you-what-do-you-usually-rely-on-when-choosing-colors-do-you-stick-with-rcolorbrewer-pick-manually-or-have-you-tried-cols4all",
    "title": "Colors Matter: Exploring cols4all for Better Data Visualization",
    "section": "Back to you: What do you usually rely on when choosing colors? Do you stick with RColorBrewer, pick manually, or have you tried cols4all?",
    "text": "Back to you: What do you usually rely on when choosing colors? Do you stick with RColorBrewer, pick manually, or have you tried cols4all?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Winkle Lu",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n     winklelu1226@gmail.com\n  \n\n      \nWith over a decade of experience in clinical trial programming, I specialize in CDISC standards and regulatory deliverable â€” but Iâ€™m not standing still. Iâ€™ve embraced open-source tools like R, Shiny, and Python to drive automation and improve data visualization.\nğŸ“˜ Blog/Sharing | ğŸ‘‰ Presentations\n\n\nClinical Research Organization | Pharmaceutical Company | Statistical Programming\n\n\n\nMaster of Public Health | Tzu Chi University\n\n\n\n\n11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for.\n\n\n\n\n\nAllergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis\n\n\n\n\n\nWinkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time â€“ discussion by view of first-time quality, PharmaSUG â€“ Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428â€“435 [SCI]."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Winkle Lu",
    "section": "",
    "text": "Clinical Research Organization | Pharmaceutical Company | Statistical Programming"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Winkle Lu",
    "section": "",
    "text": "Master of Public Health | Tzu Chi University"
  },
  {
    "objectID": "index.html#performance-summary",
    "href": "index.html#performance-summary",
    "title": "Winkle Lu",
    "section": "",
    "text": "11+ years of clinical trial programming experience with deep expertise in SDTM, ADaM, and regulatory submission.\nLed an 18-member team to complete COVID-19 Phase III programming within 3 months; results published in NEJM.\nSkilled at cross-functional collaboration, providing medical-monitor support and creating publication-ready output.\nDeveloped automation tools including an aCRF mapping system and review validation tools, boosting efficiency by 70%.\nPresented at R/Pharma 2024 and ShinyConf 2025, demonstrating advanced R Shiny proficiency.\nRecognized for consistently delivering high-quality work with exceptional efficiency, I have earned promotions at nearly every company I have worked for."
  },
  {
    "objectID": "index.html#therapeutic-area",
    "href": "index.html#therapeutic-area",
    "title": "Winkle Lu",
    "section": "",
    "text": "Allergy / Immunology: Allergic Rhinitis\nCardiovascular: Cardiovascular Disease\nCOVID-19\nDermatology: Actinic keratosis, Angiosarcoma of Skin (disorder), Preventing Hypertrophic Scar, Psoriasis\nEndocrinology: Diabetes Mellitus Type 2\nNephrology: Renal Impairment\nOncology: Multiple myeloma, Non-Small Cell Lung Cancer, Small Cell Carcinoma of Lung, Hepatocellular Carcinoma, Gastrointestinal Cancer, Malignant Melanoma, Malignant Neoplastic Disease\nTransplantation: Rheumatoid Arthritis"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Winkle Lu",
    "section": "",
    "text": "Winkle Lu, Reviewing Clinical Data Efficiently with Shiny, ShinyConf 2025.\nWinkle Lu, Presenting Clinical Results via CDISC-Compliant Shiny Apps, R/Pharma 2024.\nZhi-Sheng Lu, 2018, Combining quality of productivity and efficiency under highly pressure of lacking time â€“ discussion by view of first-time quality, PharmaSUG â€“ Beijing.\nShu-Hui Wen, Zhi-Sheng Lu, 2011, Factors affecting the effective number of tests in genetic association studies: A comparative study of three PCA-based methods, Journal of Human Genetics, 56, 428â€“435 [SCI]."
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#introduction",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ§© Introduction",
    "text": "ğŸ§© Introduction\nWhen presenting analysis results, itâ€™s often necessary to embed specific values into structured sentences â€” for example, describing results for certain patients, countries, or hospitals.\nThe glue package in R provides a powerful solution for this. It allows you to integrate variable values into a fixed text template, making your code more efficient, readable, and less prone to errors.\nIn this post, Iâ€™ll demonstrate how to: - Use glue() to dynamically create descriptive text\n- Use glue_collapse() to collapse grouped records\n- Use lapply() to manage and prefix multiple datasets\n- Integrate the results into Quarto or R Markdown reports"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#required-packages",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ“¦ Required Packages",
    "text": "ğŸ“¦ Required Packages\nlibrary(dplyr) \nlibrary(glue) \nğŸª‡ Step 1: Create and Prefix Multiple Datasetsrepresenting different hospitalsâ€™ medication records:\n# Sample CM data from different hospitals\ncm1 &lt;- data.frame(USUBJID = c(\"001\", \"002\"), CMTRT = c(\"DrugA\", \"DrugB\"))\ncm2 &lt;- data.frame(USUBJID = c(\"003\", \"004\"), CMTRT = c(\"DrugC\", \"DrugD\"))\n\n# Put into a named list\ncm_list &lt;- list(Hosp1 = cm1, Hosp2 = cm2)\n\n# Prefix each dataset with its group name\ncm_prefixed &lt;- lapply(names(cm_list), function(name) {\n  df &lt;- cm_list[[name]]\n  df$Group &lt;- name\n  df\n})\n\n# Merge all into one\ncm_all &lt;- bind_rows(cm_prefixed)"
  },
  {
    "objectID": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "href": "blog/02-JAN-2025-Generate Dynamic Text Results with glue.html#step-2-collapse-records-per-subject",
    "title": "Generate Dynamic Text Results with glue and lapply in R",
    "section": "ğŸ§  Step 2: Collapse Records per Subject",
    "text": "ğŸ§  Step 2: Collapse Records per Subject\nWe want to describe each subjectâ€™s treatment history by combining multiple CMTRT values:\n# Example: combine multiple treatments per subject\ncm_text &lt;- cm_all %&gt;%\n  group_by(USUBJID, Group) %&gt;%\n  summarise(cmx_CMTRT = glue_collapse(CMTRT, sep = \"; \"), .groups = \"drop\")\nâœ¨ Step 3: Use glue() to Format Sentences\n# Create dynamic sentences for reporting\ncm_text &lt;- cm_text %&gt;%\n  mutate(sentence = glue(\"Subject {USUBJID} in {Group} was treated with: {cmx_CMTRT}.\"))\n\n# Preview\ncm_text$sentence\nğŸ“ Output Example\nSubject 001 in Hosp1 was treated with: DrugA.\nSubject 002 in Hosp1 was treated with: DrugB.\nSubject 003 in Hosp2 was treated with: DrugC.\nSubject 004 in Hosp2 was treated with: DrugD.\nğŸ§µ Conclusion\nThe combination of glue(), glue_collapse(), and lapply() offers a powerful workflow for:\n\nfficient text generation\nFlexible dataset processing\nClean report integration\n\nThis approach not only reduces manual effort but also ensures consistency and clarity in reporting."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "",
    "text": "The Lunar New Year holiday has begun â€” Happy Year of the Snake! ğŸ‰ğŸğŸ‰\nIn many Eastern cultures, the zodiac (ç”Ÿè‚–) plays an important role in tradition, storytelling, and even personal identity. This 12-year cycle assigns an animal to each year, with the animal of your birth year becoming your zodiac sign. For example, those born in 1989 or 2001 are Snakes ğŸ, while others may be Tigers ğŸ… or Rabbits ğŸ‡.\nBut how exactly can we figure out someoneâ€™s zodiac animal using code?\nSure, you could just Google itâ€¦ ğŸ˜‚\nBut as an R programmer, I decided to use this as a small coding challenge!"
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#the-idea",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ” The Idea",
    "text": "ğŸ” The Idea\nWeâ€™ll base our logic on the Common Era (CE) calendar, and apply a combination of vectors and the {hash} package in R to map a given year to its corresponding zodiac animal.\n\nğŸ’¡ Note: For more precise results, especially if youâ€™re working with historical or cultural data, you might want to factor in the lunar calendar (which usually starts in late January or early February). For this example, weâ€™ll keep things simple."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#how-the-chinese-zodiac-works",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ§® How the Chinese Zodiac Works",
    "text": "ğŸ§® How the Chinese Zodiac Works\nThe 12 animals, in order, are:\n\nRat\n\nOx\n\nTiger\n\nRabbit\n\nDragon\n\nSnake\n\nHorse\n\nGoat\n\nMonkey\n\nRooster\n\nDog\n\nPig\n\nThe cycle repeats every 12 years. For instance, the year 2025 will be the Year of the Snake, just like 2013, 2001, 1989, etc."
  },
  {
    "objectID": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "href": "blog/01-FEB-2025-Lets Code the Zodiac in R.html#coding-it-in-r",
    "title": "ğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ",
    "section": "ğŸ’» Coding It in R",
    "text": "ğŸ’» Coding It in R\nLetâ€™s write a simple function using the {hash} package:\n# Load the hash package library(hash)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Two Simple Ways to Fill Dummy Data into the Right Rows\n\n\n\nR\n\nTidyverse\n\n\n\nTwo Simple Ways to Fill Dummy Data into the Right Rows.\n\n\n\n\n\nNov 12, 2025\n\n\n\n\n\n\n\nA Journey into Data Visualization: From ggplot2 Techniques to Visual Design\n\n\n\nR\n\nggplot2\n\nVisualization\n\nLearning\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\n\n\n\n\n\nColors Matter: Exploring cols4all for Better Data Visualization\n\n\n\nR\n\nggplot2\n\nData Visualization\n\nColors\n\n\n\nHow the R package cols4all helps you balance aesthetics and accessibility in choosing color palettes.\n\n\n\n\n\nSep 13, 2025\n\n\n\n\n\n\n\nTakeaways from Learning Programming and AI Tools - Part 1\n\n\n\nProgramming\n\nAI\n\nLearning\n\n\n\nIntroduction\n\n\n\n\n\nJul 30, 2025\n\n\n\n\n\n\n\nLesser-known but crucial role - Statistical Programmer\n\n\n\nStatistical Programmer\n\nCareer\n\n\n\n\n\n\n\n\nJul 6, 2025\n\n\n\n\n\n\n\nPlot-Table Highlighting in Shiny\n\n\n\nR\n\nDT\n\nShiny\n\nplotly\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\nCollaborating with ChatGPT in Coding\n\n\n\nAI\n\nChatGPT\n\nProgramming\n\n\n\nWhat Iâ€™ve Learned So Far\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\nğŸ§§ Happy Lunar New Year! Letâ€™s Code the Zodiac in R ğŸ\n\n\n\nR\n\nhash\n\nZodiac\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\nGenerate Dynamic Text Results with glue and lapply in R\n\n\n\nR\n\nTidyverse\n\nglue\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nEfficiently Apply the Same Function to Multiple Datasets in R\n\n\n\nR\n\nlapply\n\ngsub\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "href": "blog/04-MAY-2025-Plot-Table Highlighting in Shiny.html",
    "title": "Plot-Table Highlighting in Shiny",
    "section": "",
    "text": "When working with large volumes of clinical trial data, itâ€™s easy to get lost in hundredsâ€”or even thousandsâ€”of records. To support more intuitive data review, Iâ€™ve recently been exploring ways to combine visual plots with interactive data tables in Shiny. This approach can help reviewers quickly grasp key insights and trace them back to the raw data.\nOne idea I tested recently is a â€œhighlight recordâ€ function, which links points on a plot to specific rows in a data listing. This functionality is built using three powerful R packages: {ggplot2}, {plotly}, and {DT}.\nğŸ” How It Works: Interactive Highlighting Step-by-Step Hereâ€™s a breakdown of the mechanism:\n\nğŸ•¯ï¸ Step 1: Create the Base Plot with ggplot2\nThe visualization starts with a standard ggplot chartâ€”for example, plotting subject-level events by date and domain. This gives us full control over the aesthetics and data structure.\n\n\nğŸ•¯ï¸ Step 2: Make the Plot Interactive with plotly\nNext, I use ggplotly() from the {plotly} package to convert the static ggplot into an interactive chart. With this transformation, the plot becomes clickable and dynamic, enabling deeper user interaction.\n\n\nğŸ•¯ï¸ Step 3: Capture User Clicks Using event_data(â€œplotly_clickâ€)\nThanks to plotly_click event data, I can capture exactly which point a user clicks onâ€”such as the domain, event date, or subject ID. This click event generates metadata we can use to match against the underlying dataset.\n\n\nğŸ•¯ï¸ Step 4: Highlight the Matched Record in DT::datatable()\nThe final step is to link the clicked point to a specific row in the data table rendered with the {DT} package. When a match is found (based on selected key values like date and domain), the corresponding row is automatically highlighted, drawing the reviewerâ€™s attention to the source record.\n\n\nğŸš€ Why This Matters\nThis interaction model significantly improves the reviewer experience:\nVisual-first exploration: Users can spot patterns and anomalies visually.\nSeamless data tracing: Clicking on a point takes you straight to the corresponding recordâ€”no need to scroll through the full table.\nFaster reviews: Especially useful when reviewing patient timelines, safety events, or domain-specific findings.\n\n\nâš ï¸ One Caveat: Handle Factor Conversion with Care\nOne challenge I encountered involves the matching logic. Since ggplot may internally convert character variables (like domain) into factors, you must be very cautious when comparing click event values to original data values.\nA mismatchâ€”say, due to different data types or formattingâ€”can easily break the highlight feature. This is a common area for debugging, especially when your plot aesthetics depend on factor() transformations or custom labeling.\n\n\nFinal Thoughts\nThis approach is still evolving, but the integration of plot interactivity and table linking is already proving to be a valuable enhancement for Shiny apps, especially in clinical data review contexts. If youâ€™re working with multi-domain datasets or timeline-based visualizations, this pattern might be worth exploring. If you have built something similar or have ideas to improve this logic, I would love to hear your thoughts! â€”"
  },
  {
    "objectID": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "href": "blog/05-APR-2025-Collaborating with ChatGPT.html",
    "title": "Collaborating with ChatGPT in Coding",
    "section": "",
    "text": "Lately, Iâ€™ve been exploring how to work more effectively with ChatGPT when writing codeâ€”mostly in R and Pythonâ€”to boost my productivity and reach my goals faster.\nTo be honest, it hasnâ€™t always been smooth sailing. While AI can be a powerful assistant, working with it efficiently takes practice and intention. Iâ€™m still learning and fine-tuning my process, but Iâ€™ve picked up a few lessons along the way that Iâ€™d like to share. And of course, Iâ€™d love to hear your thoughts too ğŸª‡\nğŸ’¡ 1. Define the Programming Goal Clearly Before asking AI for help, Iâ€™ve found it crucial to clearly explain what Iâ€™m trying to achieve. The more specific and outcome-oriented the request, the more helpful the response. When I start with a well-defined goal, ChatGPT can often propose a clean structure or even a solid template for the task.\nğŸ’¡ 2. Understand the AI-Generated Code This might sound obvious, but itâ€™s tempting to copy and paste without fully understanding the AIâ€™s output. In reality, taking the time to read and grasp the logic is essential. It helps me make sure the code aligns with my intentâ€”and gives me a much better chance at troubleshooting if something goes wrong later on.\nğŸ’¡ 3. Debug Step by Step Initially, I asked ChatGPT to revise big chunks of code at once, but that often led to confusion. Iâ€™ve since learned that breaking things down into smaller parts works much better. By reviewing and applying changes step by step, I stay in control and reduce the risk of introducing new errors. It also allows me to better evaluate the AIâ€™s reasoning behind each suggestion.\nğŸ’¡ 4. Restart If the Conversation Gets Stuck There are times when ChatGPT just doesnâ€™t seem to â€œget itâ€â€”no matter how I phrase my question. In those cases, Iâ€™ve found it helpful to summarize the issue clearly and start a fresh conversation. A clean slate often results in clearer, more accurate responses.\nğŸš§ Still a Work in Progress Iâ€™m still experimenting, reflecting, and learning through this process. For me, working with AI has been more than just about writing codeâ€”itâ€™s been a way to sharpen how I think and communicate as a developer.\nIf youâ€™ve had similar experiencesâ€”or totally different onesâ€”Iâ€™d love to hear how you collaborate with AI when coding. Letâ€™s share ideas and make this learning journey smarter, together. ğŸ”ğŸ”ğŸ”"
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«-what-is-a-statistical-programmer-sp",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«-what-is-a-statistical-programmer-sp",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ” ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸« | What is a Statistical Programmer (SP)",
    "text": "ğŸ” ä»€éº¼æ˜¯çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸« | What is a Statistical Programmer (SP)\nåœ¨è‡¨åºŠè©¦é©—çš„éç¨‹ä¸­ï¼Œçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«è² è²¬å°‡è©¦é©—æ•¸æ“šä¾ç…§è¦ç¯„æ•´ç†æˆå¯ä»¥è§£é‡‹ä¸”é©åˆå ±å‘Šå‘ˆç¾çš„æ ¼å¼ã€‚é€™è£¡æ‰€èªªçš„ã€Œè¦ç¯„ã€ï¼ŒåŒ…å«å¯©æŸ¥ä¸»ç®¡æ©Ÿé—œï¼ˆä¾‹å¦‚ï¼šFDAã€EMAï¼‰æ‰€è¦æ±‚çš„æäº¤è¦æ ¼ã€CDISC åˆ¶å®šçš„æ¨™æº–ï¼ˆå¦‚ SDTMã€ADaMã€define.xmlï¼‰ï¼Œä»¥åŠå…¬å¸å…§éƒ¨çš„ä½œæ¥­è¦ç¯„ã€‚\nIn clinical trials, Statistical Programmers are responsible for organizing trial data according to predefined standards, transforming it into interpretable and report-ready formats. These â€œstandardsâ€ include regulatory submission requirements (such as those from the FDA and EMA), CDISC-defined structures like SDTM, ADaM, and define.xml, as well as internal company-specific guidelines.\né€™å€‹å•é¡Œï¼Œå…¶å¯¦æ˜¯èº«ç‚ºçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«å‘èº«æ—è¦ªå‹è§£é‡‹å·¥ä½œå…§å®¹æ™‚ï¼Œæœ€é›£èªªæ˜çš„ä¸€éƒ¨åˆ†ï¼Œå› ç‚ºå¾ˆå®¹æ˜“èˆ‡ Data Managementã€Statistician ç­‰è·å‹™æ··æ·†ã€‚ä¸éä¹Ÿæ­£å› å¦‚æ­¤ï¼Œçµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«èˆ‡ Data Managementã€Statistician çš„å·¥ä½œç¢ºå¯¦ç¶“å¸¸å¯†ä¸å¯åˆ†ã€‚\nThis is often one of the most difficult aspects for Statistical Programmers to explain to friends and family, as it is easily confused with roles like Data Management or Statistician. However, this overlap also highlights how closely intertwined these roles truly are.\næˆ‘åœ¨è‡¨åºŠè©¦é©—é ˜åŸŸæ“”ä»»çµ±è¨ˆç¨‹å¼è¨­è¨ˆå¸«è¶…éåå¹´çš„æ™‚é–“ï¼Œå¾…éå…©é–“å¤§å‹ CRO å…¬å¸èˆ‡å…©é–“è—¥å» ã€‚ä»¥ä¸‹æˆ‘å°‡ç°¡å–®åˆ†äº«ä¸€äº›å¾è‡ªå·±è§’åº¦å‡ºç™¼ï¼Œå°é€™ä»½å·¥ä½œå…§å®¹èˆ‡åƒ¹å€¼çš„è§€å¯Ÿèˆ‡ç†è§£ã€‚\nI have worked as a Statistical Programmer in the clinical trial field for over ten years, across two large CRO companies and two pharmaceutical firms. In the following sections, Iâ€™ll share my perspective on the nature and value of this profession based on my own experiences."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-åœ¨åšä»€éº¼-what-does-a-statistical-programmer-actually-do",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#statistical-programmer-åœ¨åšä»€éº¼-what-does-a-statistical-programmer-actually-do",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ” Statistical Programmer åœ¨åšä»€éº¼ | What Does a Statistical Programmer Actually Do",
    "text": "ğŸ” Statistical Programmer åœ¨åšä»€éº¼ | What Does a Statistical Programmer Actually Do\nå…ˆå‰æ“”ä»»é¢è©¦ä¸»ç®¡çš„æ™‚å€™ï¼Œå¸¸æœƒè½åˆ°é¢è©¦è€…æåˆ° CDISCã€‚CDISC æ˜¯ä»€éº¼å‘¢ï¼Ÿç°¡å–®ä¾†èªªï¼Œæ˜¯é‡å°è‡¨åºŠæ•¸æ“šåˆ¶å®šç›¸é—œäº¤æ›èˆ‡æ•´ç†æ¨™æº–çš„çµ„ç¹”ã€‚è€Œå…¶ä¸­ SDTMã€ADaM çš„æ¨™æº–åŒ–è¦ç¯„èˆ‡å»ºè­°å°±æ˜¯ç”± CDISC åˆ¶å®šå‡ºä¾†çš„ã€‚é€™å…¶å¯¦å°±æ˜¯ SP æ ¸å¿ƒå·¥ä½œçš„é‡è¦æ¨™æº–ä¹‹ä¸€ã€‚\nWhen I previously served as an interview manager, I often heard candidates mention CDISC. So, what exactly is CDISC? Simply, it is an organization that defines data exchange and standardization frameworks for clinical trial data. The well-known SDTM and ADaM standards were established by CDISC - and these are among the key foundations of a Statistical Programmerâ€™s role.\nCDISC æ˜¯æˆ‘éå¸¸å–œæ­¡çš„å–®ä½ä¹‹ä¸€ï¼Œå› ç‚ºè‡¨åºŠåŸå§‹æ•¸æ“šå…¶å¯¦æ˜¯éå¸¸å¤šå…ƒçš„ã€‚é€™å€‹çµ„ç¹”é‡å°è‡¨åºŠè©¦é©—å„éšæ®µçš„æ•¸æ“šé€²è¡Œæ¨™æº–åŒ–æ•´ç†ï¼Œè®“è³‡æ–™åœ¨æ•´ç†ã€åˆ†æã€ä»¥åŠæäº¤æ–¹é¢æ›´å…·æ•ˆç‡ã€‚\nCDISC is one of my favorite organizations, because clinical raw data is inherently diverse and complex. By applying standardized structures to each phase of a clinical trial, CDISC enables much more efficient data organization, analysis, and regulatory submission.\n\nä»¥ä¸€èˆ¬çš„æµç¨‹ä¾†èªªï¼Œç•¶è‡¨åºŠæ•¸æ“šä¾ç…§ CRFï¼ˆCase Report Formï¼‰é€²è¡Œæ”¶é›†è‡³ EDCï¼ˆElectronic Data Captureï¼‰ç³»çµ±å¾Œï¼ŒSP å°‡ä¾ç…§æ¡ˆå­çš„é€²åº¦èˆ‡éœ€æ±‚ï¼Œé–‹å§‹é€²è¡Œ SDTM æ•¸æ“šé›†çš„ç¨‹å¼ç·¨å¯«ã€‚åŒæ™‚é–“ï¼Œä¹Ÿæœƒèˆ‡ç›¸é—œéƒ¨é–€ï¼Œä¾‹å¦‚çµ±è¨ˆèˆ‡é†«å­¸éƒ¨é–€ï¼Œç¢ºèªçµ±è¨ˆåˆ†æçš„å…§å®¹å¾Œï¼Œé–‹å§‹é€²è¡Œ ADaM åŠ TLFï¼ˆTable, Listing, Figureï¼‰çš„æº–å‚™ã€‚\nIn a general workflow, once clinical data is collected by CRFs (Case Report Forms) and entered into the EDC (Electronic Data Capture) system, SPs begin programming the SDTM datasets based on the project timeline and needs. At the same time, they collaborate with key departments - such as statisticians and medical reviewers - to confirm the analysis plan and begin developing ADaM datasets and generating TLFs (Tables, Listings, and Figures).\nå›æ†¶è‡ªå·±å‰›é€²é€™é ˜åŸŸæ™‚ï¼Œé‚£æ™‚å€™çš„éƒ¨é–€åˆ†å·¥ååˆ†ç´°ï¼ŒSDTM èˆ‡ ADaM+TLF æ˜¯ç”±ä¸åŒçš„ SP åœ˜éšŠè² è²¬ã€‚æˆ‘ä¸€é–‹å§‹æ˜¯åœ¨è² è²¬ ADaM+TLF çš„åœ˜éšŠï¼Œç•¶æ™‚ç¬¬ä¸€å€‹ä¸»è¦ä»»å‹™æ˜¯ ADLB æ•¸æ“šé›†ã€‚ç”±æ–¼æ•¸é‡é¾å¤§ã€åˆ†ææ–¹æ³•è¤‡é›œï¼Œç¨‹å¼æ¯æ¬¡åŸ·è¡Œéƒ½è¦è€—æ™‚ 4ï½5 å°æ™‚ã€‚\nLooking back to when I first entered the field, the departments were highly specialized - separate teams handled SDTM and ADaM+TLF. I started in the ADaM+TLF team, and my first major task was the ADLB (laboratory) dataset. Due to its size and the complexity of the analysis methods, each run of the program would take 4 to 5 hours.\nè¨˜å¾—é‚£æ™‚å€™åœ˜éšŠå¸¸å¸¸ä¸€èµ·æ™šé¤å¾Œåˆä¸€èµ·åŠ ç­ï¼Œå°±é€™æ¨£æŒçºŒäº†å¹¾å€‹æœˆï¼Œæœ€å¾Œé †åˆ©å®Œæˆ Sponsor çš„éœ€æ±‚ï¼Œæ¥è‘—ä¹Ÿæ¨é€²è‡³ define.xml çš„è£½ä½œã€‚å°ç•¶æ™‚å¹´è³‡é‚„ä¸åˆ°ä¸€å¹´çš„æˆ‘ä¾†èªªï¼Œèƒ½æ¥è§¸é€™æ¨£çš„ä»»å‹™å¯¦å±¬é›£å¾—ï¼Œå“ˆå“ˆã€‚\nI remember how the team often stayed late after dinner to keep working together. After several months, we successfully met the sponsorâ€™s expectations and moved on to preparing define.xml. At the time, I had less than one year of experience, so being involved in such tasks was rare - and rewarding - for a junior SP.\né›–ç„¶ç¬¬ä¸€å¹´çš„ SP ç”Ÿæ´»è®“æˆ‘åƒç›¡è‹¦é ­ï¼Œä¸éåœ¨ç•¶æ™‚ mentor å’Œåœ˜éšŠçš„å¸¶é ˜ä¸‹ï¼Œä¹Ÿè®“æˆ‘æ›´å¿«æ‰“ä¸‹ SP ç”Ÿæ¶¯çš„é‡è¦åŸºç¤ã€‚\nAlthough that first year as a Statistical Programmer was filled with challenges, the guidance of my mentor and the support of the team helped me quickly build a solid foundation in this career."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#sp-èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©—-statistical-programmers-in-todays-clinical-trials",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#sp-èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©—-statistical-programmers-in-todays-clinical-trials",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "âœ¨ SP èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©— | Statistical Programmers in Todayâ€™s Clinical Trials",
    "text": "âœ¨ SP èˆ‡ç¾åœ¨çš„è‡¨åºŠè©¦é©— | Statistical Programmers in Todayâ€™s Clinical Trials\nå‰›å…¥è¡Œæ™‚ï¼ŒSP å·¥ä½œå¤šæ•¸ä»¥ SAS ç‚ºä¸»è¦çš„å¸¸ç”¨ç¨‹å¼èªè¨€ã€‚SAS æ˜¯ä¸€é …éå¸¸ç©©å®šçš„å·¥å…·ï¼Œæˆ‘èªç‚ºæœ€å¼·å¤§çš„éƒ¨åˆ†æ˜¯å®ƒèƒŒå¾Œçš„æ”¯æŒå…¬å¸è³‡æºã€‚å³ä½¿é‡åˆ° SAS æœ¬èº«çš„æŠ€è¡“å•é¡Œï¼Œä¹Ÿæœƒæœ‰å°ˆæ¥­åœ˜éšŠå”åŠ©è§£æ±ºï¼›å¦å¤–ï¼Œç”±æ–¼é€™æ˜¯è‡¨åºŠè©¦é©—é ˜åŸŸé•·ä¹…ä»¥ä¾†æ‰€ä½¿ç”¨çš„èªè¨€ï¼Œå¯ä¾›åƒè€ƒçš„ç¨‹å¼ç¯„ä¾‹éå¸¸è±å¯Œï¼Œé€™ä¹Ÿæ˜¯è‡³ä»Š SAS æ“æœ‰ä¸å¯æ’¼å‹•åœ°ä½çš„åŸå› ä¹‹ä¸€ã€‚\nWhen I first entered the field, SAS was the dominant programming language for Statistical Programmers. It is a highly stable tool, and in my view, its greatest strength lies in the robust support from the company behind it. Even when technical issues arise, professional support teams are available to help. Moreover, as SAS has been widely used in clinical trials for decades, it offers an abundance of reference programs â€” one of the key reasons for its long-standing, unshakable position in the field,\nå°è±¡ä¸­ï¼Œå¾ 2018 å¹´é–‹å§‹ï¼Œæˆ‘é–‹å§‹æœ‰æ©Ÿæœƒåƒèˆ‡å…¬å¸å¤–çš„ç ”è¨æœƒï¼Œopen-source tool ç›¸é—œçš„åˆ†äº«å·²ç¶“å¾ˆå¤šï¼Œä¾‹å¦‚ï¼šRã€Pythonã€‚é€™æ˜¯éå¸¸å¥½çš„ç¾è±¡ï¼Œä»£è¡¨æ•´å€‹è‡¨åºŠè©¦é©—é ˜åŸŸæ­£åœ¨ä¸æ–·æ€è€ƒèˆ‡é€²æ­¥ã€‚å°æˆ‘ä¾†èªªï¼Œé€™äº›å·¥å…·ä¸æ‡‰è©²è¢«ç”¨ä¾†èˆ‡ SAS ç›¸äº’æ¯”è¼ƒæˆ–ç«¶çˆ­ï¼Œè€Œæ˜¯æä¾›çµ¦ä½¿ç”¨è€…ï¼ˆä¾‹å¦‚ SPï¼‰æ›´å¤šå”ä½œçš„å¯èƒ½æ€§èˆ‡æ€è€ƒçš„ç©ºé–“ã€‚\nI recall that starting around 2018, I had the opportunity to attend external conferences, and there were already many discussions around open-source tools like R and Python. This is a very positive trend - it shows that the clinical trial industry is continuously evolving and open to new ideas. Personally, I donâ€™t see these tools as competitors to SAS. Rather, they offer more collaborative options and new perspectives for users like Statistical Programmers."
  },
  {
    "objectID": "blog/06-JUL-2025-Statistical Programmer.html#è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨-sp-çš„åˆ©å™¨-what-are-the-current-must-have-skills-for-sps",
    "href": "blog/06-JUL-2025-Statistical Programmer.html#è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨-sp-çš„åˆ©å™¨-what-are-the-current-must-have-skills-for-sps",
    "title": "Lesser-known but crucial role - Statistical Programmer",
    "section": "ğŸ’¡ è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨ SP çš„åˆ©å™¨ | What Are the Current Must-Have Skills for SPs",
    "text": "ğŸ’¡ è‡³æ–¼ä»€éº¼æ˜¯ç¾åœ¨ SP çš„åˆ©å™¨ | What Are the Current Must-Have Skills for SPs\nå¦‚æœç”±æˆ‘ä¾†å›ç­”ï¼Œæˆ‘æœƒåˆ—èˆ‰ä»¥ä¸‹å¹¾é»ï¼š\nIf I were to answer this question, here are the capabilities I consider essential:\nğŸ§©ç†è§£ç•¶ä¸‹æ‰€é€²è¡Œçš„ä»»å‹™: SP ç¶“æ‰‹è¨±å¤šä»»å‹™ä¹Ÿå¸¸æœƒè·Ÿå…¶ä»–éƒ¨é–€åˆä½œ, ç†è§£ç•¶ä¸‹ä»»å‹™çš„åŸç”±é™¤äº†å¯ä»¥å¹«åŠ©ç¶­æŒæ­£ç¢ºé”æˆç›®æ¨™å¤–, ä¹Ÿå¯ä»¥é©æ™‚æå‡ºæ›´åˆé©çš„è§£æ±ºæ–¹æ¡ˆ\nUnderstanding the task at hand: SPs handle various tasks and frequently collaborate with other departments. Understanding the purpose behind the current task not only helps ensure that the objectives are met correctly, but also allows SPs to propose more appropriate solutions when needed.\nğŸ§©ç¶­æŒé«˜å“è³ªçš„ç”¢å‡º: è‡¨åºŠè©¦é©—ç›´æ¥é—œä¹äººé«”ç”Ÿå‘½ï¼Œæ˜¯ä¸€ä»½éœ€è¦å¯©æ…å°å¾…çš„å·¥ä½œï¼›è‹¥å› å“è³ªä¸ä½³è€Œå¤šæ¬¡ä¿®æ”¹ï¼Œå°‡ç›´æ¥å½±éŸ¿æ¡ˆå­çš„æ•´é«”æ™‚ç¨‹\nDelivering high-quality outputs: Clinical trials involve human lives, and this work must be approached with great care. Poor quality that requires repeated revisions can directly delay project timelines.\nğŸ§©ç†Ÿæ‚‰å¸¸è¦å·¥ä½œçš„æŠ€èƒ½: äº†è§£æ¯å€‹å¸¸è¦æ­¥é©Ÿï¼Œä¸¦é«˜æ•ˆç‡å®Œæˆå·¥ä½œï¼Œæ˜¯é™ä½é¢¨éšªèˆ‡æå‡ç©©å®šæ€§çš„é—œéµ\nMastering routine tasks: Understanding each standard step and performing them efficiently helps reduce risk and ensure consistent deliverables.\nğŸ§©ç¶­æŒæ€è€ƒèˆ‡æ‹“å±•è¦–é‡: SP çš„å¸¸è¦ç¨‹å¼é–‹ç™¼éœ€æ±‚å±¬æ–¼ä¸­ç­‰ç¨‹åº¦ï¼Œä½†è‹¥èƒ½ä¸æ–·æ€è€ƒæ”¹å–„æµç¨‹ï¼Œä¸¦é—œæ³¨ç”¢æ¥­è¶¨å‹¢ï¼Œå°‡æ˜¯æå‡è‡ªæˆ‘åƒ¹å€¼æœ€æœ‰æ•ˆçš„æ–¹å¼\nKeeping a growth mindset: While the programming complexity for routine SP tasks is moderate, constantly seeking improvements and tracking industry developments is one of the fastest ways to increase oneâ€™s value.\nğŸ§©AI çš„æ‡‰ç”¨: å»¶çºŒå‰ä¸€é»ï¼ŒAI çš„æµªæ½®å·²ç¶“å¹é€²è‡¨åºŠè©¦é©—é ˜åŸŸã€‚AI ä¸åƒ…èƒ½å„ªåŒ–ç¨‹å¼é–‹ç™¼æµç¨‹ï¼Œä¹Ÿå¯èƒ½æ”¹å–„ SP çš„è¡Œæ”¿èˆ‡æºé€šæµç¨‹ã€‚ç›¸é—œè¨è«–æ­£æŒçºŒç™¼å±•ä¸­ï¼Œéå¸¸å€¼å¾—æ€è€ƒèˆ‡åƒè€ƒ\nLeveraging AI tools: Building on the previous point - the wave of AI has already entered the clinical trial space. AI has the potential to enhance both programming and administrative workflows for SPs. These discussions are ongoing, and I highly recommend staying informed and open-minded."
  }
]